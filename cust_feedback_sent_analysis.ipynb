{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f4be2-1f69-48aa-b5db-140297bf3470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a0d5870-3d04-4c7c-bc01-c9ff228f8d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 120 reviews from C:\\Users\\victor\\Downloads\\customer_reviews.csv\n",
      "Loaded 2006 entries from C:\\Users\\victor\\Desktop\\VIC\\dsc\\positive-words.txt\n",
      "Loaded 4783 entries from C:\\Users\\victor\\Desktop\\VIC\\dsc\\negative-words.txt\n",
      "Saved processed reviews to customer_reviews_processed_by_you.csv\n",
      "Saved summary by product to summary_by_product_by_you.csv\n",
      "Wrote Excel dashboard to customer_feedback_dashboard_by_you.xlsx\n",
      "Saved sentiment distribution chart to sentiment_distribution_by_you.png\n",
      "Saved avg rating chart to avg_rating_by_product_by_you.png\n",
      "Processing complete. Check the generated CSV/Excel/PNG files in the current folder.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "customer_feedback_sentiment.py\n",
    "\n",
    "Run in Jupyter or as a script. Adjust the input paths if needed.\n",
    "\n",
    "What it does (high level):\n",
    "- Reads a raw CSV of reviews (customer_reviews.csv).\n",
    "- Loads large positive/negative lexicon files (text files).\n",
    "- Cleans and tokenizes each review.\n",
    "- Matches multi-word or single-word lexicon entries as whole-phrase matches.\n",
    "- Applies simple negation handling (flip polarity if a negation occurs within a small window).\n",
    "- Counts positive/negative matches, normalizes to a sentiment score, assigns a label.\n",
    "- Saves processed CSV, summary CSV, Excel workbook and two PNG charts.\n",
    "\n",
    "Edit the variables in the CONFIG section below to point to your files.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Set\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------- CONFIG: adjust these paths for your machine -----------------\n",
    "RAW_REVIEWS_CSV = r\"C:\\Users\\victor\\Downloads\\customer_reviews.csv\"\n",
    "POSITIVE_LEXICON_PATH = r\"C:\\Users\\victor\\Desktop\\VIC\\dsc\\positive-words.txt\"\n",
    "NEGATIVE_LEXICON_PATH = r\"C:\\Users\\victor\\Desktop\\VIC\\dsc\\negative-words.txt\"\n",
    "\n",
    "# Fallback path if the exact windows path is not found (useful for testing here)\n",
    "FALLBACK_POS_PATH = \"positive-words.txt\"   # change if you put the file elsewhere\n",
    "FALLBACK_NEG_PATH = \"negative-words.txt\"\n",
    "\n",
    "# Output filenames (will be created in the current working directory)\n",
    "OUT_PROCESSED_CSV = \"customer_reviews_processed_by_you.csv\"\n",
    "OUT_SUMMARY_CSV = \"summary_by_product_by_you.csv\"\n",
    "OUT_EXCEL = \"customer_feedback_dashboard_by_you.xlsx\"\n",
    "OUT_SENT_PNG = \"sentiment_distribution_by_you.png\"\n",
    "OUT_RATING_PNG = \"avg_rating_by_product_by_you.png\"\n",
    "\n",
    "# Parameters you might want to tune\n",
    "NEGATION_WINDOW = 3     # how many tokens before a matched phrase to look for a negation\n",
    "POSITIVE_THRESHOLD = 0.08\n",
    "NEGATIVE_THRESHOLD = -0.05\n",
    "\n",
    "# ----------------- Helper functions -----------------\n",
    "def try_open_lexicon(path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Read a lexicon file, ignoring comment lines (starting with ';' or '#') and blank lines.\n",
    "    Attempt utf-8 first, then latin-1 (common for older lexicon files).\n",
    "    Returns a list of raw lines (not yet cleaned).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Lexicon file not found: {path}\")\n",
    "\n",
    "    for encoding in (\"utf-8\", \"latin-1\"):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=encoding) as f:\n",
    "                lines = [ln.rstrip(\"\\n\") for ln in f]\n",
    "            # filter out comment header lines and blank lines\n",
    "            items = []\n",
    "            for ln in lines:\n",
    "                ln_stripped = ln.strip()\n",
    "                if not ln_stripped:\n",
    "                    continue\n",
    "                if ln_stripped.startswith(\";\") or ln_stripped.startswith(\"#\"):\n",
    "                    continue\n",
    "                items.append(ln_stripped)\n",
    "            return items\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    # If we reach here, raise a helpful error\n",
    "    raise UnicodeDecodeError(f\"Cannot decode lexicon file {path} with utf-8 or latin-1.\")\n",
    "\n",
    "\n",
    "_re_non_alnum = re.compile(r\"[^a-z0-9\\s]\")  # compile once\n",
    "\n",
    "def clean_text_to_string(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text for phrase matching:\n",
    "    - Lowercase\n",
    "    - Replace non-alphanumeric characters with spaces\n",
    "    - Collapse multiple spaces\n",
    "    - Surround with single spaces (padding) to enable whole-phrase substring matching.\n",
    "    Returns cleaned string, e.g. \" this is text \"\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        s = \"\"\n",
    "    s = str(s).lower()\n",
    "    s = _re_non_alnum.sub(\" \", s)         # replace punctuation with space\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()   # collapse multi-space\n",
    "    return f\" {s} \"                       # pad with spaces\n",
    "\n",
    "def phrase_to_clean(phrase: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a lexicon phrase into the same cleaned-padded format used for review text.\n",
    "    For example \"fast delivery\" -> \" fast delivery \"\n",
    "    \"\"\"\n",
    "    p = phrase.lower()\n",
    "    p = _re_non_alnum.sub(\" \", p)\n",
    "    p = re.sub(r\"\\s+\", \" \", p).strip()\n",
    "    return f\" {p} \"\n",
    "\n",
    "def find_phrase_positions(tokens: List[str], phrase_tokens: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Return list of start indices in tokens where phrase_tokens exactly match.\n",
    "    Example: tokens = ['i','love','it','fast','delivery']; phrase_tokens = ['fast','delivery'] -> [3]\n",
    "    This is token-level detection (used if you choose token-based approach).\n",
    "    Note: we primarily use cleaned-string substring matching (below) for simplicity with multi-word phrases.\n",
    "    \"\"\"\n",
    "    positions = []\n",
    "    L = len(phrase_tokens)\n",
    "    if L == 0:\n",
    "        return positions\n",
    "    for i in range(len(tokens) - L + 1):\n",
    "        if tokens[i:i+L] == phrase_tokens:\n",
    "            positions.append(i)\n",
    "    return positions\n",
    "\n",
    "# ----------------- Core sentiment function -----------------\n",
    "class SentimentLexicon:\n",
    "    def __init__(self, positive_list: List[str], negative_list: List[str]):\n",
    "        \"\"\"\n",
    "        Prepare lexicon: create cleaned phrase -> original phrase mapping,\n",
    "        and order entries by descending token length so longer phrases match first.\n",
    "        \"\"\"\n",
    "        # Clean each entry and ignore empty lines\n",
    "        def prepare(list_in):\n",
    "            cleaned = []\n",
    "            for ln in list_in:\n",
    "                ln2 = ln.strip()\n",
    "                if not ln2:\n",
    "                    continue\n",
    "                c = phrase_to_clean(ln2)    # \" phrase words \"\n",
    "                # store pairs (cleaned_phrase, token_length)\n",
    "                token_length = len(c.split())\n",
    "                cleaned.append((c, token_length, ln2))\n",
    "            # sort by token_length descending to match longer phrases first\n",
    "            cleaned.sort(key=lambda x: -x[1])\n",
    "            return cleaned\n",
    "\n",
    "        self.pos = prepare(positive_list)\n",
    "        self.neg = prepare(negative_list)\n",
    "\n",
    "        # small set of negation words (expandable)\n",
    "        self.negations = {\"not\",\"no\",\"never\",\"none\",\"cannot\",\"can't\",\"dont\",\"don't\",\"didnt\",\"didn't\",\"doesnt\",\"doesn't\",\"won't\",\"wouldn't\",\"couldn't\",\"isn't\",\"isnt\"}\n",
    "\n",
    "    def analyze_review(self, raw_text: str, negation_window:int = NEGATION_WINDOW) -> Tuple[int,int,int,List[Tuple[str,str,int]]]:\n",
    "        \"\"\"\n",
    "        Analyze a single review text.\n",
    "        Returns:\n",
    "         - pos_count (int)\n",
    "         - neg_count (int)\n",
    "         - token_count (int)\n",
    "         - matches (list of tuples): (matched_phrase_orig, polarity('pos'/'neg'), start_char_index_in_cleaned_text)\n",
    "        Explanation:\n",
    "         - We do substring checks on a cleaned, padded string to detect multi-word phrases as whole-phrases.\n",
    "         - We avoid double-counting overlapping matched tokens by tracking used character index ranges.\n",
    "         - We apply simple negation handling: if a negation word appears within `negation_window` tokens BEFORE the matched phrase, the phrase polarity flips.\n",
    "        \"\"\"\n",
    "        cleaned = clean_text_to_string(raw_text)  # e.g. \" i love it fast delivery \"\n",
    "        token_count = 0\n",
    "        # token_count approximate: count words in cleaned (strip spaces then split)\n",
    "        token_count = len(cleaned.strip().split())\n",
    "\n",
    "        used_char_indices: Set[int] = set()\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        matches = []\n",
    "\n",
    "        # helper: find token index of char position to check negation window\n",
    "        cleaned_tokens = cleaned.strip().split()  # no external punctuation now\n",
    "        # create a mapping from token index to char pos in cleaned string\n",
    "        # (we can compute approximate token positions by splitting the cleaned string)\n",
    "        cum_chars = []\n",
    "        cpos = 1  # because cleaned string starts with a space we added; tokenization uses strip\n",
    "        for t in cleaned_tokens:\n",
    "            cum_chars.append(cpos)\n",
    "            cpos += len(t) + 1  # token + single space\n",
    "\n",
    "        # function to check negation within window tokens before a token index (word idx)\n",
    "        def has_negation_before_word(word_idx:int) -> bool:\n",
    "            # check previous negation_window tokens\n",
    "            start = max(0, word_idx - negation_window)\n",
    "            for idx in range(start, word_idx):\n",
    "                if cleaned_tokens[idx] in self.negations:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        # A function to mark used char ranges (avoid overlap)\n",
    "        def range_overlaps_any(start_char:int, length:int) -> bool:\n",
    "            for ch in range(start_char, start_char + length):\n",
    "                if ch in used_char_indices:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        def mark_range(start_char:int, length:int):\n",
    "            for ch in range(start_char, start_char + length):\n",
    "                used_char_indices.add(ch)\n",
    "\n",
    "        # We'll search positive phrases first, then negative phrases.\n",
    "        # This order is a design choice: long positive phrases should match before short negative tokens\n",
    "        # to reduce accidental partial overlaps. The lists are pre-sorted by token length descending.\n",
    "        for cleaned_list, polarity in ((self.pos, \"pos\"), (self.neg, \"neg\")):\n",
    "            for cleaned_phrase, _, orig_phrase in cleaned_list:\n",
    "                # cleaned_phrase includes leading/trailing space, cleaned\n",
    "                # find all substring positions\n",
    "                start_idx = 0\n",
    "                while True:\n",
    "                    found_at = cleaned.find(cleaned_phrase, start_idx)\n",
    "                    if found_at == -1:\n",
    "                        break\n",
    "                    # ensure we don't double-count overlapping matches\n",
    "                    if range_overlaps_any(found_at, len(cleaned_phrase)):\n",
    "                        # skip this occurrence and continue searching after it\n",
    "                        start_idx = found_at + 1\n",
    "                        continue\n",
    "\n",
    "                    # Determine the approximate word index where phrase starts:\n",
    "                    # count how many spaces are before found_at in cleaned\n",
    "                    # (since cleaned has single spaces) -> token_index = number of tokens before this match\n",
    "                    # We'll use split on the substring up to found_at\n",
    "                    prefix = cleaned[:found_at].strip()\n",
    "                    if prefix == \"\":\n",
    "                        word_idx = 0\n",
    "                    else:\n",
    "                        word_idx = len(prefix.split())\n",
    "\n",
    "                    # negation check: if negation present within the previous NEGATION_WINDOW tokens, flip polarity\n",
    "                    negated = has_negation_before_word(word_idx)\n",
    "\n",
    "                    # Decide final polarity for counting\n",
    "                    if polarity == \"pos\":\n",
    "                        if negated:\n",
    "                            # counts as negative\n",
    "                            neg_count += 1\n",
    "                            matches.append((orig_phrase, \"pos->neg\", found_at))\n",
    "                        else:\n",
    "                            pos_count += 1\n",
    "                            matches.append((orig_phrase, \"pos\", found_at))\n",
    "                    else:  # polarity == \"neg\"\n",
    "                        if negated:\n",
    "                            # negation of negative -> becomes positive\n",
    "                            pos_count += 1\n",
    "                            matches.append((orig_phrase, \"neg->pos\", found_at))\n",
    "                        else:\n",
    "                            neg_count += 1\n",
    "                            matches.append((orig_phrase, \"neg\", found_at))\n",
    "\n",
    "                    # mark char range as used and move start_idx forward\n",
    "                    mark_range(found_at, len(cleaned_phrase))\n",
    "                    start_idx = found_at + len(cleaned_phrase)\n",
    "\n",
    "        return pos_count, neg_count, token_count, matches\n",
    "\n",
    "# ----------------- Main processing routine -----------------\n",
    "def process_reviews(csv_path: str,\n",
    "                    pos_lex_path: str,\n",
    "                    neg_lex_path: str,\n",
    "                    out_processed_csv: str = OUT_PROCESSED_CSV,\n",
    "                    out_summary_csv: str = OUT_SUMMARY_CSV,\n",
    "                    out_excel: str = OUT_EXCEL):\n",
    "    # 1) Read reviews CSV\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"Reviews CSV not found at: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path, parse_dates=[\"review_date\"])\n",
    "    print(f\"Loaded {len(df)} reviews from {csv_path}\")\n",
    "\n",
    "    # 2) Load lexicons (with helpful fallbacks)\n",
    "    def load_safe(path, fallback):\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                items = try_open_lexicon(path)\n",
    "                print(f\"Loaded {len(items)} entries from {path}\")\n",
    "                return items\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {path}: {e}\")\n",
    "                # try fallback if available\n",
    "        if os.path.exists(fallback):\n",
    "            try:\n",
    "                items = try_open_lexicon(fallback)\n",
    "                print(f\"Loaded {len(items)} entries from fallback {fallback}\")\n",
    "                return items\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to read both {path} and fallback {fallback}: {e}\")\n",
    "        raise FileNotFoundError(f\"Neither {path} nor fallback {fallback} exist.\")\n",
    "\n",
    "    pos_list = load_safe(pos_lex_path, FALLBACK_POS_PATH)\n",
    "    neg_list = load_safe(neg_lex_path, FALLBACK_NEG_PATH)\n",
    "\n",
    "    # 3) Initialize lexicon analyzer\n",
    "    lex = SentimentLexicon(pos_list, neg_list)\n",
    "\n",
    "    # 4) Compute sentiment for each review. We'll create columns:\n",
    "    #    sentiment_score, sentiment_label, pos_count, neg_count, token_count\n",
    "    results = []\n",
    "    for i, text in enumerate(df['review_text'].fillna(\"\").astype(str)):\n",
    "        pos_c, neg_c, token_c, matches = lex.analyze_review(text, negation_window=NEGATION_WINDOW)\n",
    "        score = (pos_c - neg_c) / max(1, token_c)\n",
    "        if score > POSITIVE_THRESHOLD:\n",
    "            label = \"positive\"\n",
    "        elif score < NEGATIVE_THRESHOLD:\n",
    "            label = \"negative\"\n",
    "        else:\n",
    "            label = \"neutral\"\n",
    "        results.append({\n",
    "            \"pos_count\": pos_c,\n",
    "            \"neg_count\": neg_c,\n",
    "            \"token_count\": token_c,\n",
    "            \"sentiment_score\": round(score, 4),\n",
    "            \"sentiment_label\": label,\n",
    "            # (optional) keep a small summary of matches for debugging - join first few matches:\n",
    "            \"match_preview\": \"; \".join([f\"{m[0]}|{m[1]}\" for m in matches[:6]])\n",
    "        })\n",
    "        # (Optional) progress print every 200 rows\n",
    "        if (i+1) % 200 == 0:\n",
    "            print(f\"Processed {i+1} reviews...\")\n",
    "\n",
    "    # join results into dataframe\n",
    "    rdf = pd.DataFrame(results)\n",
    "    df_out = pd.concat([df.reset_index(drop=True), rdf.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # 5) Save processed CSV\n",
    "    df_out.to_csv(out_processed_csv, index=False)\n",
    "    print(f\"Saved processed reviews to {out_processed_csv}\")\n",
    "\n",
    "    # 6) Aggregate summary by product\n",
    "    summary = df_out.groupby(['product_id','product_name']).agg(\n",
    "        total_reviews = ('review_id','count'),\n",
    "        avg_rating = ('rating','mean'),\n",
    "        avg_sentiment_score = ('sentiment_score','mean'),\n",
    "        positive_count = ('sentiment_label', lambda x: (x=='positive').sum()),\n",
    "        neutral_count = ('sentiment_label', lambda x: (x=='neutral').sum()),\n",
    "        negative_count = ('sentiment_label', lambda x: (x=='negative').sum())\n",
    "    ).reset_index()\n",
    "    summary.to_csv(out_summary_csv, index=False)\n",
    "    print(f\"Saved summary by product to {out_summary_csv}\")\n",
    "\n",
    "    # 7) Save Excel workbook with two sheets\n",
    "    with pd.ExcelWriter(out_excel, engine='openpyxl') as writer:\n",
    "        df_out.to_excel(writer, sheet_name='processed_reviews', index=False)\n",
    "        summary.to_excel(writer, sheet_name='summary_by_product', index=False)\n",
    "    print(f\"Wrote Excel dashboard to {out_excel}\")\n",
    "\n",
    "    # 8) Create two charts and save PNGs\n",
    "    # Sentiment distribution (overall)\n",
    "    sent_order = ['positive','neutral','negative']\n",
    "    sent_counts = df_out['sentiment_label'].value_counts().reindex(sent_order).fillna(0)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sent_counts.plot(kind='bar')\n",
    "    plt.title(\"Customer Review Sentiment Distribution\")\n",
    "    plt.xlabel(\"Sentiment\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_SENT_PNG)\n",
    "    plt.close()\n",
    "    print(f\"Saved sentiment distribution chart to {OUT_SENT_PNG}\")\n",
    "\n",
    "    # Average rating by product\n",
    "    plt.figure(figsize=(8,4))\n",
    "    summary_sorted = summary.sort_values('avg_rating', ascending=False)\n",
    "    plt.bar(summary_sorted['product_name'], summary_sorted['avg_rating'])\n",
    "    plt.title(\"Average Customer Rating by Product\")\n",
    "    plt.xlabel(\"Product\")\n",
    "    plt.ylabel(\"Average Rating\")\n",
    "    plt.xticks(rotation=20, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_RATING_PNG)\n",
    "    plt.close()\n",
    "    print(f\"Saved avg rating chart to {OUT_RATING_PNG}\")\n",
    "\n",
    "    # Done\n",
    "    return df_out, summary\n",
    "\n",
    "# --------------- If executed as a script, run process ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    # attempt to handle common mistakes - inform user if files are missing\n",
    "    try:\n",
    "        df_processed, summary_tbl = process_reviews(RAW_REVIEWS_CSV, POSITIVE_LEXICON_PATH, NEGATIVE_LEXICON_PATH)\n",
    "        print(\"Processing complete. Check the generated CSV/Excel/PNG files in the current folder.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error during processing:\", e)\n",
    "        print(\"Tips:\")\n",
    "        print(\"- Verify RAW_REVIEWS_CSV, POSITIVE_LEXICON_PATH and NEGATIVE_LEXICON_PATH exist and are correct.\")\n",
    "        print(\"- If lexicon files fail to decode, try opening them and saving as UTF-8 or use Latin-1.\")\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76312cc-a128-4e0f-a235-007583143901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
